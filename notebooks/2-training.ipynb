{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meat Quality Assignment - Training our model\n",
    "\n",
    "---\n",
    "\n",
    "In the second step of our project, we train a binary classification model on the images to predict whether the meat is fresh or spoiled. Here we are not so interested in the the absolute performance of the model as much in defining a clear and reusable training pipeline for image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from loguru import logger\n",
    "from sklearn.model_selection import StratifiedShuffleSplit \n",
    "\n",
    "# Set parent folder as root to import local modules\n",
    "module_path = os.path.abspath(os.path.join(\"..\"))\n",
    "sys.path.append(module_path)\n",
    "\n",
    "# Remove default logger and set level to INFO\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"INFO\")\n",
    "\n",
    "from src.base.model import CNNModel\n",
    "from src.base.pipelines import images_to_frame, decode_image, augment_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(module_path, \"data/meat-quality-assessment-based-on-deep-learning/\")\n",
    "output_path = os.path.join(module_path, \"notebooks/output/training\")\n",
    "os.makedirs(output_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_img = images_to_frame(data_path=data_path)\n",
    "df_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_img.drop(columns=[\"label\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = 0.1\n",
    "test_size = (df_img.shape[0] * valid_size) / (df_img.shape[0] - df_img.shape[0] * valid_size)\n",
    "valid_split = StratifiedShuffleSplit(n_splits=1, test_size=valid_size, random_state=42)\n",
    "test_split = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=42)\n",
    "\n",
    "[(optim_idx, valid_idx)] = [i for i in valid_split.split(X=np.zeros([df_img.shape[0], 1]), y=df_img[\"target\"], groups=None)]\n",
    "[(train_idx, test_idx)] = [i for i in test_split.split(X=np.zeros([len(optim_idx), 1]), y=df_img.loc[optim_idx, \"target\"], groups=None)]\n",
    "\n",
    "train_df = df_img.loc[train_idx, :]\n",
    "test_df = df_img.loc[test_idx, :]\n",
    "valid_df = df_img.loc[valid_idx, :]\n",
    "\n",
    "logger.info(f\"Training set size: {len(train_idx):,.0f}, test set size: {len(test_idx):,.0f}, validation set size: {len(valid_idx):,.0f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "RESIZE_SHAPE = (256, 256)\n",
    "SHUFFLE_SIZE = 512\n",
    "N_EPOCHS = 5\n",
    "STEPS_PER_EPOCH = np.ceil(train_df.shape[0] / BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((train_df[\"image_path\"], train_df[\"target\"]))\n",
    "    .map(partial(decode_image, image_size=RESIZE_SHAPE, n_labels=2), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    .map(augment_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    .repeat()\n",
    "    .shuffle(512)\n",
    "    .batch(32)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "\n",
    "valid_set = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((valid_df[\"image_path\"], valid_df[\"target\"]))\n",
    "    .map(partial(decode_image, image_size=RESIZE_SHAPE, n_labels=2), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    .batch(32)\n",
    "    .cache()\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t, l in train_set.take(1):\n",
    "    print(t.numpy().shape, l.numpy().shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNModel(width=4, height=4, name='CNNModel')\n",
    "model.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(0.001),\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits = True),\n",
    "    metrics = [tf.keras.losses.BinaryCrossentropy(from_logits = True, name = 'BCE'), 'accuracy']\n",
    ")\n",
    "model.build(input_shape=(None, 256, 256, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(  \n",
    "    train_set, \n",
    "    epochs = N_EPOCHS,\n",
    "    verbose = 1,\n",
    "    steps_per_epoch = STEPS_PER_EPOCH,\n",
    "    validation_data = valid_set\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('meat-quality-assessment-m4CLWqOE-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "cd17a26abef4eb4c850ff5b519a824c392533d39a236404cf53d52af12209a12"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
